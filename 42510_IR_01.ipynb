{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment No: 01\n",
        "## Aim: Write a program for pre-processing of a text document such as stop word removal, stemming."
      ],
      "metadata": {
        "id": "S9bglK71WGHG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdqcyqmsUWam",
        "outputId": "911535a9-850f-48e3-ee71-cd32a862ad2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text: ['natur', 'languag', 'process', 'nlp', 'subfield', 'artifici', 'intellig', 'ai', 'focus', 'interact', 'comput', 'human', 'natur', 'languag', 'ultim', 'goal', 'nlp', 'read', 'deciph', 'understand', 'make', 'sens', 'human', 'languag', 'valuabl', 'way']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 1. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 2. Convert to lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # 3. Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # 4. Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # 5. Stemming\n",
        "    ps = PorterStemmer()\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Sample text document\n",
        "text_document = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI)\n",
        "that focuses on the interaction between computers and humans through natural language.\n",
        "The ultimate goal of NLP is to read, decipher, understand, and make sense of human\n",
        "language in a valuable way.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess the text\n",
        "processed_text = preprocess_text(text_document)\n",
        "print(\"Processed Text:\", processed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation**\n",
        "\n",
        "**Tokenization:** Splits the text\n",
        "into individual words (tokens) using word_tokenize.\n",
        "\n",
        "**Convert to lowercase:** Normalizes the tokens to lowercase for consistency.\n",
        "\n",
        "**Remove punctuation:** Filters out punctuation marks from the tokens.\n",
        "\n",
        "**Remove stop words:** Uses a set of English stop words to filter out common words that add little value (e.g., \"is\", \"the\", \"and\").\n",
        "\n",
        "**Stemming:** Reduces words to their root form using the Porter Stemmer."
      ],
      "metadata": {
        "id": "G6V_WqycV1kD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "poM67i6rUjxb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}